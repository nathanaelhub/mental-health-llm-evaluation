#!/usr/bin/env python3
"""
Final Demo: Real Model Selection and Evaluation Working
Shows the clean system with NO mock responses, only real LLM evaluation
"""
import requests
import json
import time

BASE_URL = "http://localhost:8000"

def test_conversation(title, prompt, user_id):
    """Test a single conversation with real model selection"""
    print(f"\n{'='*70}")
    print(f"ğŸ§  {title}")
    print(f"{'='*70}")
    print(f"ğŸ’¬ USER: {prompt}")
    print("â³ Running REAL model evaluation (this takes 60-90 seconds)...")
    
    start = time.time()
    
    try:
        response = requests.post(
            f"{BASE_URL}/api/chat",
            json={"message": prompt, "user_id": user_id},
            timeout=120
        )
        
        duration = time.time() - start
        
        if response.status_code == 200:
            data = response.json()
            
            print(f"âš¡ EVALUATION COMPLETE ({duration:.1f}s)")
            print(f"ğŸ¯ WINNER: {data['selected_model'].upper()}")
            print(f"ğŸ“Š CONFIDENCE: {data['confidence_score']:.1%}")
            
            if data.get('model_scores'):
                print(f"ğŸ“‹ SCORES: {data['model_scores']}")
            
            print(f"ğŸ¤– {data['selected_model'].upper()}: {data['response']}")
            
            return data
        else:
            print(f"âŒ Error {response.status_code}: {response.text}")
            return None
            
    except requests.exceptions.Timeout:
        print(f"âŒ Request timed out after {time.time() - start:.1f}s")
        return None
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def main():
    print("ğŸ­ REAL MODEL EVALUATION DEMO")
    print("ğŸ¯ Authentic therapeutic model selection - NO MOCK RESPONSES")
    print("âœ… All responses generated by real LLM models")
    print("ğŸ“Š Models compete based on actual therapeutic evaluation scores")
    
    # Test different types of therapeutic conversations
    conversations = [
        ("ANXIETY SUPPORT", "I have been having panic attacks at work every day this week and I don't know how to cope", "anxiety_user"),
        ("DEPRESSION ASSISTANCE", "I've lost all motivation and everything feels pointless. I can barely get out of bed", "depression_user"), 
        ("CRISIS INTERVENTION", "I'm having thoughts of self-harm and feel like nobody would care if I disappeared", "crisis_user"),
        ("RELATIONSHIP GUIDANCE", "My marriage is falling apart and we fight about everything. I don't know how to fix it", "relationship_user"),
        ("STRESS MANAGEMENT", "I'm overwhelmed with work deadlines and family responsibilities. Everything feels too much", "stress_user")
    ]
    
    results = []
    
    for title, prompt, user_id in conversations:
        result = test_conversation(title, prompt, user_id)
        if result:
            results.append({
                'title': title,
                'selected_model': result['selected_model'],
                'confidence': result['confidence_score'],
                'scores': result.get('model_scores', {}),
                'prompt_type': result.get('prompt_type', 'unknown')
            })
        
        # Brief pause between tests
        time.sleep(2)
    
    # Summary
    print(f"\n{'='*70}")
    print(f"ğŸ“ˆ EVALUATION SUMMARY")
    print(f"{'='*70}")
    
    if results:
        model_wins = {}
        for r in results:
            model_wins[r['selected_model']] = model_wins.get(r['selected_model'], 0) + 1
        
        print(f"ğŸ† MODEL PERFORMANCE:")
        for model, wins in sorted(model_wins.items(), key=lambda x: x[1], reverse=True):
            print(f"   â€¢ {model.upper()}: {wins} selections")
        
        print(f"\nğŸ“Š DETAILED RESULTS:")
        for r in results:
            print(f"   â€¢ {r['title']}: {r['selected_model'].upper()} ({r['confidence']:.1%} confidence)")
            if r['scores']:
                scores_str = ', '.join([f"{k}: {v:.1f}" for k, v in r['scores'].items()])
                print(f"     Scores: {scores_str}")
        
        print(f"\nâœ… DEMONSTRATION COMPLETE!")
        print(f"   ğŸ¯ {len(results)} real evaluations performed")
        print(f"   ğŸ¤– {len(model_wins)} different models selected")
        print(f"   ğŸ“Š All scores based on authentic therapeutic evaluation")
        print(f"   âŒ Zero mock responses - everything is real!")
        
    else:
        print("âŒ No successful evaluations completed")
        print("ğŸ’¡ Check server logs and model availability")

if __name__ == "__main__":
    main()