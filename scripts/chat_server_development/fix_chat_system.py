#!/usr/bin/env python3
"""
Mental Health Chat System Repair & Validation Script
===================================================

This script comprehensively tests, diagnoses, fixes, and validates the 
mental health chat system to ensure all components work correctly.

Usage:
    python scripts/fix_chat_system.py                    # Test and diagnose only
    python scripts/fix_chat_system.py --apply-fixes      # Test, diagnose, and fix
    python scripts/fix_chat_system.py --validate-only    # Run validation tests only
"""

import sys
import os
import asyncio
import time
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import traceback

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    import aiohttp
    import requests
    from dotenv import load_dotenv
    from src.chat.dynamic_model_selector import DynamicModelSelector
    from src.chat.conversation_session_manager import ConversationSessionManager, MessageRole
    from src.chat.persistent_session_store import SessionStoreType
    from src.models.openai_client import OpenAIClient
    from src.models.claude_client import ClaudeClient
    from src.models.deepseek_client import DeepSeekClient
    from src.models.gemma_client import GemmaClient
except ImportError as e:
    print(f"‚ùå Import Error: {e}")
    print("Make sure you're running from the project root and dependencies are installed")
    sys.exit(1)

load_dotenv()

class ChatSystemDoctor:
    """Comprehensive chat system diagnostics and repair"""
    
    def __init__(self):
        self.project_root = project_root
        self.issues_found = []
        self.fixes_applied = []
        self.test_results = {}
        
        # Default configuration
        self.models_config = {
            'models': {
                'openai': {'enabled': True, 'cost_per_token': 0.0001, 'model_name': 'gpt-4o-mini'},
                'claude': {'enabled': True, 'cost_per_token': 0.00015, 'model_name': 'claude-3-5-sonnet-20241022'},
                'deepseek': {'enabled': True, 'cost_per_token': 0.00005, 'model_name': 'deepseek/deepseek-r1-0528-qwen3-8b'},
                'gemma': {'enabled': True, 'cost_per_token': 0.00003, 'model_name': 'google/gemma-3-12b'}
            },
            'default_model': 'openai',
            'selection_timeout': 40.0,
            'similarity_threshold': 0.9
        }
        
        self.test_prompts = [
            "I'm feeling anxious about work and need support",
            "I'm so depressed and feel hopeless", 
            "What are some good coping strategies?",
            "I want to hurt myself"
        ]
    
    def log_issue(self, issue: str, severity: str = "ERROR"):
        """Log an issue found during diagnosis"""
        self.issues_found.append({"issue": issue, "severity": severity, "timestamp": datetime.now()})
        if severity == "ERROR":
            print(f"‚ùå {issue}")
        elif severity == "WARNING":
            print(f"‚ö†Ô∏è  {issue}")
        else:
            print(f"‚ÑπÔ∏è  {issue}")
    
    def log_fix(self, fix: str):
        """Log a fix that was applied"""
        self.fixes_applied.append({"fix": fix, "timestamp": datetime.now()})
        print(f"‚úÖ {fix}")
    
    async def test_current_state(self):
        """Test 1: Comprehensive system state analysis"""
        print("\nüîç TESTING CURRENT SYSTEM STATE")
        print("=" * 50)
        
        # Test environment configuration
        await self.test_environment_config()
        
        # Test model availability
        await self.test_model_availability()
        
        # Test timeout configurations
        await self.test_timeout_settings()
        
        # Test session management
        await self.test_session_management()
        
        # Test API endpoints
        await self.test_api_endpoints()
    
    async def test_environment_config(self):
        """Test environment configuration and API keys"""
        print("\nüîß Testing Environment Configuration...")
        
        # Check .env file
        env_file = self.project_root / ".env"
        if not env_file.exists():
            self.log_issue("Missing .env file", "ERROR")
            return
        
        # Check API keys
        api_keys = {
            'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),
            'ANTHROPIC_API_KEY': os.getenv('ANTHROPIC_API_KEY'),
            'DEEPSEEK_API_KEY': os.getenv('DEEPSEEK_API_KEY'),
            'GEMMA_API_KEY': os.getenv('GEMMA_API_KEY'),
        }
        
        missing_keys = []
        for key, value in api_keys.items():
            if not value or value == "your_api_key_here":
                missing_keys.append(key)
            else:
                print(f"‚úÖ {key}: Configured")
        
        if missing_keys:
            self.log_issue(f"Missing/invalid API keys: {', '.join(missing_keys)}", "WARNING")
        
        # Check local endpoints
        local_server = os.getenv('LOCAL_LLM_SERVER', '192.168.86.23:1234')
        try:
            response = requests.get(f'http://{local_server}/v1/models', timeout=5)
            if response.status_code == 200:
                models = response.json().get('data', [])
                model_names = [m['id'] for m in models]
                print(f"‚úÖ Local server accessible: {len(models)} models available")
                print(f"   Available: {', '.join(model_names)}")
                self.test_results['local_server'] = {'status': 'available', 'models': model_names}
            else:
                self.log_issue(f"Local server returned {response.status_code}", "WARNING")
        except Exception as e:
            self.log_issue(f"Local server not accessible: {e}", "WARNING")
            self.test_results['local_server'] = {'status': 'unavailable', 'error': str(e)}
    
    async def test_model_availability(self):
        """Test individual model client availability and response times"""
        print("\nü§ñ Testing Model Availability...")
        
        model_clients = {
            'openai': OpenAIClient,
            'claude': ClaudeClient,
            'deepseek': DeepSeekClient,
            'gemma': GemmaClient
        }
        
        self.test_results['models'] = {}
        test_prompt = "Hello, this is a test message"
        
        for model_name, ClientClass in model_clients.items():
            print(f"\nüîπ Testing {model_name.upper()}...")
            
            try:
                # Initialize client
                start_init = time.time()
                client = ClientClass()
                init_time = (time.time() - start_init) * 1000
                
                # Test response generation
                start_response = time.time()
                
                if asyncio.iscoroutinefunction(client.generate_response):
                    response = await asyncio.wait_for(
                        client.generate_response(
                            prompt=test_prompt,
                            system_prompt="You are a helpful assistant."
                        ),
                        timeout=30.0
                    )
                else:
                    response = client.generate_response(
                        prompt=test_prompt,
                        system_prompt="You are a helpful assistant."
                    )
                
                response_time = (time.time() - start_response) * 1000
                
                if hasattr(response, 'error') and response.error:
                    self.log_issue(f"{model_name} returned error: {response.error}", "ERROR")
                    self.test_results['models'][model_name] = {
                        'status': 'error',
                        'error': response.error,
                        'init_time_ms': init_time
                    }
                elif hasattr(response, 'content') and response.content:
                    content_preview = response.content[:50] + "..." if len(response.content) > 50 else response.content
                    print(f"  ‚úÖ Success: {response_time:.1f}ms - {content_preview}")
                    self.test_results['models'][model_name] = {
                        'status': 'working',
                        'init_time_ms': init_time,
                        'response_time_ms': response_time,
                        'content_length': len(response.content)
                    }
                else:
                    self.log_issue(f"{model_name} returned empty response", "ERROR")
                    self.test_results['models'][model_name] = {
                        'status': 'empty_response',
                        'init_time_ms': init_time,
                        'response_time_ms': response_time
                    }
                
            except asyncio.TimeoutError:
                self.log_issue(f"{model_name} timed out after 30 seconds", "ERROR")
                self.test_results['models'][model_name] = {
                    'status': 'timeout',
                    'timeout_seconds': 30
                }
            except Exception as e:
                self.log_issue(f"{model_name} failed: {str(e)}", "ERROR")
                self.test_results['models'][model_name] = {
                    'status': 'error',
                    'error': str(e)
                }
    
    async def test_timeout_settings(self):
        """Test current timeout configurations"""
        print("\n‚è∞ Testing Timeout Settings...")
        
        # Check model selector timeout
        try:
            selector = DynamicModelSelector(self.models_config)
            current_timeout = selector.selection_timeout
            
            if current_timeout < 30.0:
                self.log_issue(f"Model selection timeout too short: {current_timeout}s (recommended: 40s)", "WARNING")
            else:
                print(f"‚úÖ Model selection timeout: {current_timeout}s")
            
            self.test_results['timeouts'] = {
                'selection_timeout': current_timeout,
                'recommended_minimum': 40.0
            }
            
        except Exception as e:
            self.log_issue(f"Failed to check timeout settings: {e}", "ERROR")
    
    async def test_session_management(self):
        """Test session management functionality"""
        print("\nüìö Testing Session Management...")
        
        try:
            session_manager = ConversationSessionManager(
                store_type=SessionStoreType.MEMORY,
                enable_safety_monitoring=True,
                enable_audit_trail=True
            )
            
            # Test session creation
            test_session = await session_manager.create_session(
                user_id="test_user",
                selected_model="openai",
                initial_message="Test session"
            )
            
            # Test message adding
            await session_manager.add_message(
                test_session.session_id,
                MessageRole.USER,
                "Test message"
            )
            
            # Test session retrieval
            retrieved_session = await session_manager.get_session(test_session.session_id)
            
            if retrieved_session and len(retrieved_session.conversation_history) > 0:
                print("‚úÖ Session management working correctly")
                self.test_results['session_management'] = {'status': 'working'}
            else:
                self.log_issue("Session management not storing messages correctly", "ERROR")
                self.test_results['session_management'] = {'status': 'broken'}
                
        except Exception as e:
            self.log_issue(f"Session management failed: {e}", "ERROR")
            self.test_results['session_management'] = {'status': 'error', 'error': str(e)}
    
    async def test_api_endpoints(self):
        """Test API endpoint availability"""
        print("\nüåê Testing API Endpoints...")
        
        # Test if server is running
        endpoints_to_test = [
            ('GET', 'http://localhost:8000/', 'Root endpoint'),
            ('GET', 'http://localhost:8000/api/status', 'Status endpoint'),
            ('GET', 'http://localhost:8000/api/models/status', 'Model status endpoint'),
        ]
        
        self.test_results['api_endpoints'] = {}
        
        for method, url, description in endpoints_to_test:
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    print(f"‚úÖ {description}: Available")
                    self.test_results['api_endpoints'][url] = {'status': 'available', 'code': response.status_code}
                else:
                    self.log_issue(f"{description} returned {response.status_code}", "WARNING")
                    self.test_results['api_endpoints'][url] = {'status': 'error', 'code': response.status_code}
            except Exception as e:
                self.log_issue(f"{description} not accessible: {e}", "WARNING")
                self.test_results['api_endpoints'][url] = {'status': 'unreachable', 'error': str(e)}
    
    async def identify_issues(self):
        """Analyze test results and identify specific issues"""
        print("\nüî¨ IDENTIFYING ISSUES")
        print("=" * 50)
        
        # Analyze model performance
        working_models = [name for name, result in self.test_results.get('models', {}).items() 
                         if result.get('status') == 'working']
        
        if len(working_models) < 2:
            self.log_issue("Less than 2 models working - insufficient for comparison", "ERROR")
        elif len(working_models) < 4:
            failed_models = [name for name, result in self.test_results.get('models', {}).items() 
                           if result.get('status') != 'working']
            self.log_issue(f"Some models not working: {', '.join(failed_models)}", "WARNING")
        
        # Analyze timeout issues
        slow_models = []
        for name, result in self.test_results.get('models', {}).items():
            if result.get('response_time_ms', 0) > 25000:  # 25 seconds
                slow_models.append(f"{name} ({result['response_time_ms']/1000:.1f}s)")
        
        if slow_models:
            self.log_issue(f"Slow models detected: {', '.join(slow_models)}", "WARNING")
        
        # Check if server is running
        server_accessible = any(
            result.get('status') == 'available' 
            for result in self.test_results.get('api_endpoints', {}).values()
        )
        
        if not server_accessible:
            self.log_issue("Chat server not running or not accessible", "ERROR")
        
        # Summary
        print(f"\nüìä DIAGNOSIS SUMMARY:")
        print(f"   Working models: {len(working_models)}/4")
        print(f"   Issues found: {len([i for i in self.issues_found if i['severity'] == 'ERROR'])}")
        print(f"   Warnings: {len([i for i in self.issues_found if i['severity'] == 'WARNING'])}")
    
    async def apply_fixes(self):
        """Apply fixes for identified issues"""
        print("\nüîß APPLYING FIXES")
        print("=" * 50)
        
        # Fix 1: Update timeout configurations
        await self.fix_timeout_configurations()
        
        # Fix 2: Update model selector confidence calculation
        await self.fix_confidence_calculation()
        
        # Fix 3: Ensure all models are properly configured
        await self.fix_model_configurations()
        
        # Fix 4: Update server configuration
        await self.fix_server_configuration()
        
        print(f"\n‚úÖ Applied {len(self.fixes_applied)} fixes")
    
    async def fix_timeout_configurations(self):
        """Fix timeout settings in model selector"""
        print("\n‚è∞ Fixing timeout configurations...")
        
        selector_file = self.project_root / "src" / "chat" / "dynamic_model_selector.py"
        
        if selector_file.exists():
            content = selector_file.read_text()
            
            # Fix default timeout
            if "selection_timeout', 5.0)" in content:
                content = content.replace(
                    "selection_timeout', 5.0)",
                    "selection_timeout', 40.0)  # Increased for slow local models"
                )
                self.log_fix("Updated default selection timeout from 5.0s to 40.0s")
            
            # Fix individual model timeout
            if "response_timeout = 8.0" in content:
                content = content.replace(
                    "response_timeout = 8.0  # Individual model timeout",
                    "response_timeout = 35.0  # Individual model timeout (increased for local models)"
                )
                self.log_fix("Updated individual model timeout from 8.0s to 35.0s")
            
            selector_file.write_text(content)
        
        # Fix server configuration timeout
        server_file = self.project_root / "simple_server.py"
        if server_file.exists():
            content = server_file.read_text()
            
            if "'selection_timeout': 10.0" in content:
                content = content.replace(
                    "'selection_timeout': 10.0",
                    "'selection_timeout': 40.0  # Increased timeout for slow local models"
                )
                self.log_fix("Updated server selection timeout to 40.0s")
                server_file.write_text(content)
    
    async def fix_confidence_calculation(self):
        """Ensure confidence calculation is working properly"""
        print("\nüéØ Verifying confidence calculation...")
        
        selector_file = self.project_root / "src" / "chat" / "dynamic_model_selector.py"
        
        if selector_file.exists():
            content = selector_file.read_text()
            
            # Check if the new confidence calculation is present
            if "margin of victory and absolute performance" in content:
                print("‚úÖ Confidence calculation already updated")
            else:
                self.log_issue("Confidence calculation needs to be updated", "WARNING")
                # The fix has already been applied in previous session
                print("‚ÑπÔ∏è  Confidence calculation fix should be applied manually if needed")
    
    async def fix_model_configurations(self):
        """Ensure all 4 models are properly configured"""
        print("\nü§ñ Fixing model configurations...")
        
        # Check if all models are in the configuration
        expected_models = ['openai', 'claude', 'deepseek', 'gemma']
        
        try:
            selector = DynamicModelSelector(self.models_config)
            available_models = selector.get_available_models()
            
            missing_models = set(expected_models) - set(available_models)
            if missing_models:
                self.log_issue(f"Models not configured: {', '.join(missing_models)}", "WARNING")
            else:
                print("‚úÖ All 4 models configured in selector")
                self.log_fix("Verified all 4 models are configured")
                
        except Exception as e:
            self.log_issue(f"Failed to verify model configuration: {e}", "ERROR")
    
    async def fix_server_configuration(self):
        """Fix server configuration issues"""
        print("\nüñ•Ô∏è  Fixing server configuration...")
        
        server_file = self.project_root / "simple_server.py"
        
        if server_file.exists():
            content = server_file.read_text()
            
            # Ensure all 4 models are in server config
            if "'gemma': {" in content and "'deepseek': {" in content:
                print("‚úÖ Server configuration includes all models")
                self.log_fix("Verified server includes all 4 models")
            else:
                self.log_issue("Server configuration missing some models", "WARNING")
        
        # Check if UI files exist
        ui_files = [
            "src/ui/templates/chat.html",
            "src/ui/static/css/chat.css", 
            "src/ui/static/js/chat.js"
        ]
        
        for ui_file in ui_files:
            file_path = self.project_root / ui_file
            if file_path.exists():
                print(f"‚úÖ {ui_file} exists")
            else:
                self.log_issue(f"Missing UI file: {ui_file}", "WARNING")
    
    async def validate_fixes(self):
        """Validate that fixes work correctly"""
        print("\n‚úÖ VALIDATING FIXES")
        print("=" * 50)
        
        # Validation 1: Test model selection with confidence calculation
        await self.validate_model_selection()
        
        # Validation 2: Test conversation continuation
        await self.validate_conversation_flow()
        
        # Validation 3: Test API responses
        await self.validate_api_responses()
        
        print(f"\nüìä VALIDATION SUMMARY:")
        validation_results = self.test_results.get('validation', {})
        passed_tests = len([v for v in validation_results.values() if v.get('status') == 'passed'])
        total_tests = len(validation_results)
        print(f"   Validation tests passed: {passed_tests}/{total_tests}")
    
    async def validate_model_selection(self):
        """Validate model selection and confidence calculation"""
        print("\nüéØ Validating model selection...")
        
        self.test_results['validation'] = {}
        
        try:
            selector = DynamicModelSelector(self.models_config)
            
            # Test with a representative prompt
            test_prompt = "I'm feeling anxious about work and need support"
            
            start_time = time.time()
            selection = await selector.select_best_model(test_prompt)
            selection_time = (time.time() - start_time) * 1000
            
            # Validate confidence score
            if selection.confidence_score > 0.0:
                print(f"‚úÖ Confidence calculation working: {selection.confidence_score:.1%}")
                confidence_valid = True
            else:
                print(f"‚ùå Confidence score still 0%")
                confidence_valid = False
            
            # Validate reasoning
            if len(selection.selection_reasoning) > 50:
                print(f"‚úÖ Detailed reasoning provided")
                reasoning_valid = True
            else:
                print(f"‚ùå Reasoning too brief: {selection.selection_reasoning}")
                reasoning_valid = False
            
            # Validate model scores
            if len(selection.model_scores) >= 1:
                print(f"‚úÖ Model scores available: {len(selection.model_scores)} models")
                scores_valid = True
            else:
                print(f"‚ùå No model scores returned")
                scores_valid = False
            
            self.test_results['validation']['model_selection'] = {
                'status': 'passed' if all([confidence_valid, reasoning_valid, scores_valid]) else 'failed',
                'confidence_score': selection.confidence_score,
                'reasoning_length': len(selection.selection_reasoning),
                'models_evaluated': len(selection.model_scores),
                'selection_time_ms': selection_time
            }
            
        except Exception as e:
            print(f"‚ùå Model selection validation failed: {e}")
            self.test_results['validation']['model_selection'] = {
                'status': 'error',
                'error': str(e)
            }
    
    async def validate_conversation_flow(self):
        """Validate conversation continuation works"""
        print("\nüí¨ Validating conversation flow...")
        
        try:
            session_manager = ConversationSessionManager(
                store_type=SessionStoreType.MEMORY,
                enable_safety_monitoring=True,
                enable_audit_trail=True
            )
            
            # Create test session
            session = await session_manager.create_session(
                user_id="test_user",
                selected_model="openai",
                initial_message="I'm feeling anxious"
            )
            
            # Add messages
            await session_manager.add_message(
                session.session_id,
                MessageRole.USER,
                "Tell me more about anxiety"
            )
            
            await session_manager.add_message(
                session.session_id,
                MessageRole.ASSISTANT,
                "Anxiety is a normal response to stress...",
                model_used="openai"
            )
            
            # Verify session persistence
            retrieved = await session_manager.get_session(session.session_id)
            
            if retrieved and len(retrieved.conversation_history) >= 2:
                print("‚úÖ Conversation flow working correctly")
                self.test_results['validation']['conversation_flow'] = {
                    'status': 'passed',
                    'messages_stored': len(retrieved.conversation_history)
                }
            else:
                print("‚ùå Conversation flow not working")
                self.test_results['validation']['conversation_flow'] = {
                    'status': 'failed',
                    'messages_stored': len(retrieved.conversation_history) if retrieved else 0
                }
                
        except Exception as e:
            print(f"‚ùå Conversation flow validation failed: {e}")
            self.test_results['validation']['conversation_flow'] = {
                'status': 'error',
                'error': str(e)
            }
    
    async def validate_api_responses(self):
        """Validate API endpoints return correct responses"""
        print("\nüåê Validating API responses...")
        
        # Test status endpoint
        try:
            response = requests.get('http://localhost:8000/api/status', timeout=5)
            if response.status_code == 200:
                data = response.json()
                if 'status' in data and 'available_models' in data:
                    print("‚úÖ Status API working correctly")
                    api_valid = True
                else:
                    print("‚ùå Status API missing required fields")
                    api_valid = False
            else:
                print(f"‚ùå Status API returned {response.status_code}")
                api_valid = False
                
            self.test_results['validation']['api_responses'] = {
                'status': 'passed' if api_valid else 'failed',
                'status_code': response.status_code if 'response' in locals() else None
            }
            
        except Exception as e:
            print(f"‚ùå API validation failed: {e}")
            self.test_results['validation']['api_responses'] = {
                'status': 'error',
                'error': str(e)
            }
    
    def generate_report(self):
        """Generate comprehensive report of diagnosis and fixes"""
        report_file = self.project_root / "results" / "development" / "chat_system_fix_report.json"
        report_file.parent.mkdir(parents=True, exist_ok=True)
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'test_results': self.test_results,
            'issues_found': self.issues_found,
            'fixes_applied': self.fixes_applied,
            'summary': {
                'total_issues': len(self.issues_found),
                'critical_issues': len([i for i in self.issues_found if i['severity'] == 'ERROR']),
                'warnings': len([i for i in self.issues_found if i['severity'] == 'WARNING']),
                'fixes_applied': len(self.fixes_applied),
                'working_models': len([
                    name for name, result in self.test_results.get('models', {}).items() 
                    if result.get('status') == 'working'
                ])
            }
        }
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nüìã Full report saved to: {report_file}")
        return report

async def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Mental Health Chat System Doctor')
    parser.add_argument('--apply-fixes', action='store_true', help='Apply fixes after diagnosis')
    parser.add_argument('--validate-only', action='store_true', help='Run validation tests only')
    args = parser.parse_args()
    
    doctor = ChatSystemDoctor()
    
    print("üè• MENTAL HEALTH CHAT SYSTEM DOCTOR")
    print("=" * 60)
    print(f"Project: {doctor.project_root}")
    print(f"Mode: {'Fix & Validate' if args.apply_fixes else 'Validate Only' if args.validate_only else 'Diagnose Only'}")
    print("=" * 60)
    
    try:
        if not args.validate_only:
            # Step 1: Test current state
            await doctor.test_current_state()
            
            # Step 2: Identify issues
            await doctor.identify_issues()
            
            # Step 3: Apply fixes if requested
            if args.apply_fixes:
                await doctor.apply_fixes()
        
        # Step 4: Validate fixes
        await doctor.validate_fixes()
        
        # Step 5: Generate report
        report = doctor.generate_report()
        
        # Final summary
        print("\n" + "=" * 60)
        print("üéØ FINAL SUMMARY")
        print("=" * 60)
        
        summary = report['summary']
        print(f"Working models: {summary['working_models']}/4")
        print(f"Critical issues: {summary['critical_issues']}")
        print(f"Warnings: {summary['warnings']}")
        print(f"Fixes applied: {summary['fixes_applied']}")
        
        validation_results = doctor.test_results.get('validation', {})
        if validation_results:
            passed_validations = len([v for v in validation_results.values() if v.get('status') == 'passed'])
            total_validations = len(validation_results)
            print(f"Validation tests: {passed_validations}/{total_validations} passed")
        
        if summary['critical_issues'] == 0 and summary['working_models'] >= 2:
            print("\n‚úÖ SYSTEM STATUS: HEALTHY")
        elif summary['working_models'] >= 2:
            print("\n‚ö†Ô∏è  SYSTEM STATUS: FUNCTIONAL WITH WARNINGS")
        else:
            print("\n‚ùå SYSTEM STATUS: CRITICAL ISSUES DETECTED")
        
    except Exception as e:
        print(f"\n‚ùå CRITICAL ERROR: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(asyncio.run(main()))