# Mental Health LLM Evaluation - Capstone Project

**Academic capstone project comparing local vs cloud LLMs for mental health telemedicine applications.**

## üöÄ Quick Start

**Ready to evaluate in 5 minutes:**

```bash
# 1. Install dependencies
pip install openai pyyaml

# 2. Set your OpenAI API key
export OPENAI_API_KEY='${OPENAI_API_KEY}'

# 3. Run evaluation
python run_research.py --quick
```

**That's it!** Results automatically saved with statistical analysis and visualizations.

## üéØ Project Overview

This completed capstone project provides comprehensive evaluation results comparing Large Language Models (LLMs) in mental health support conversations. The research successfully evaluated **four models**: OpenAI GPT-4, Anthropic Claude, DeepSeek R1, and Gemma 7B across key therapeutic dimensions including empathy, safety, and crisis detection.

**Current Status**: Research Complete (July 2025) - Final evaluation of 4 models completed with statistical analysis

**Key Achievement**: Consolidated from 1,740+ files to **22 core files** (99% reduction) while maintaining full research functionality and statistical rigor.

### Research Focus
- **Primary Question**: How do local/open-source LLMs (DeepSeek, Gemma) compare to commercial cloud LLMs (GPT-4, Claude) in therapeutic conversation quality?
- **Secondary Questions**: Cost-benefit analysis, safety assessment, deployment feasibility across all four models
- **Study Type**: **Completed exploratory study (n=10 scenarios)** with statistical significance testing
- **Target Audience**: Academic researchers, healthcare organizations, AI/ML developers

## üèÜ Key Research Findings

### **Overall Winner: DeepSeek R1 (7.90/10)**

**Final Model Rankings:**
1. **DeepSeek R1**: 7.90/10 - Superior therapeutic performance, free local deployment
2. **OpenAI GPT-4**: 6.82/10 - Excellent safety record, reliable cloud service  
3. **Claude**: 5.41/10 - Best clarity scores, professional communication
4. **Gemma 7B**: 4.10/10 - Limited therapeutic responses, research baseline

### **Statistical Significance Achieved**
- **Composite Scores**: p < 0.05, large effect size (d = 1.33)
- **Empathy Dimension**: p < 0.05, large effect size (d = 0.58)
- **Therapeutic Value**: p < 0.05, very large effect size (d = 1.81)
- **Perfect Safety Scores**: All models achieved 10.0/10 safety ratings

### **Notable Finding: Local Model Excellence**
**DeepSeek R1 (local) nearly matches and exceeds GPT-4 performance** while offering:
- Zero per-request costs vs $0.002 for GPT-4
- Local deployment for privacy-sensitive healthcare applications
- Superior therapeutic communication quality
- Equivalent safety performance

### **Cost-Effectiveness Analysis**
- **DeepSeek**: $0.00 per response (local deployment)
- **Gemma**: $0.00 per response (local deployment)  
- **OpenAI GPT-4**: $0.002 per response
- **Claude**: $0.003 per response

## üìÅ Project Structure

```
mental-health-llm-evaluation/
‚îú‚îÄ‚îÄ scripts/                      # Main execution scripts
‚îÇ   ‚îú‚îÄ‚îÄ run_research.py           # Full evaluation pipeline
‚îÇ   ‚îú‚îÄ‚îÄ compare_models.py         # Quick model comparison
‚îÇ   ‚îú‚îÄ‚îÄ demo_presentation.py      # Academic demonstration tool
‚îÇ   ‚îú‚îÄ‚îÄ clean_results.py          # Results maintenance
‚îÇ   ‚îî‚îÄ‚îÄ development/              # Development and debug tools
‚îÇ       ‚îú‚îÄ‚îÄ debug_test.py         # Debugging utilities
‚îÇ       ‚îú‚îÄ‚îÄ reliable_research.py  # Research wrapper scripts
‚îÇ       ‚îî‚îÄ‚îÄ ...                   # Other development tools
‚îú‚îÄ‚îÄ src/                          # Modular source code
‚îÇ   ‚îú‚îÄ‚îÄ analysis/                 # Statistical analysis and visualization
‚îÇ   ‚îú‚îÄ‚îÄ config/                   # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/               # Evaluation metrics and logic
‚îÇ   ‚îú‚îÄ‚îÄ models/                   # Model interfaces and clients
‚îÇ   ‚îú‚îÄ‚îÄ research/                 # New research modules
‚îÇ   ‚îî‚îÄ‚îÄ utils/                    # Utility functions and helpers
‚îú‚îÄ‚îÄ config/                       # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ scenarios.yaml            # 10 mental health scenarios
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml               # Main configuration
‚îú‚îÄ‚îÄ data/                         # Static input data
‚îÇ   ‚îî‚îÄ‚îÄ scenarios/                # Scenario JSON files
‚îú‚îÄ‚îÄ results/                      # Final research outputs
‚îÇ   ‚îú‚îÄ‚îÄ detailed_results.json     # Complete 4-model evaluation
‚îÇ   ‚îú‚îÄ‚îÄ statistical_analysis.json # Statistical analysis results
‚îÇ   ‚îú‚îÄ‚îÄ research_report.txt       # Research findings report
‚îÇ   ‚îú‚îÄ‚îÄ RESULTS_SUMMARY.md        # Executive summary
‚îÇ   ‚îú‚îÄ‚îÄ visualizations/           # 5 publication-quality charts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1_overall_comparison.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2_category_radar.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 3_cost_effectiveness.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 4_safety_metrics.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 5_statistical_summary.png
‚îÇ   ‚îî‚îÄ‚îÄ archive/                  # Historical results
‚îî‚îÄ‚îÄ docs/                         # Documentation
    ‚îú‚îÄ‚îÄ methodology.md            # Research methodology
    ‚îî‚îÄ‚îÄ SCORING_METRICS.md        # Evaluation metrics
```

## ‚úÖ Current Capabilities

**Models Currently Supported**:
- **OpenAI GPT-4** (via API) - Fully integrated and tested
- **Anthropic Claude** (via API) - Production ready with thinking tag cleanup
- **DeepSeek R1** (via LM Studio) - Local model with session management
- **Gemma** (via LM Studio) - Local model alternative

**NEW: Therapeutic Effectiveness Scoring**:
- **4-dimension evaluation system** with weighted scoring:
  - Empathy (30%): Emotional understanding and validation
  - Therapeutic Value (25%): Helpful coping strategies and techniques
  - Safety (35%): Crisis handling and appropriate responses
  - Clarity (10%): Clear, understandable communication
- **Pattern-based detection** for consistent evaluation

**NEW: Automated Statistical Analysis**:
- **Shapiro-Wilk normality testing** for appropriate test selection
- **t-tests/Mann-Whitney U** for significance testing
- **Cohen's d effect sizes** for practical significance
- **Safety violation tracking** with detailed reporting

**COMPLETED: Publication-Quality Visualizations**:
- **5 research charts** completed showing all 4 models
- **Statistical significance indicators** with p-values and effect sizes
- **300 DPI resolution** ready for academic publication
- **All models displayed** with consistent color scheme and proper legends

**Research Quality**:
- Statistical rigor with appropriate test selection
- Publication-ready analysis and reporting
- Extensible architecture for additional models
- 5-minute setup time for immediate research use

## üõ†Ô∏è Installation & Setup

### Minimal Installation (Core Research)
```bash
# Core dependencies only
pip install -r requirements_minimal.txt
# Includes: openai, pyyaml

# Set environment variable
export OPENAI_API_KEY='${OPENAI_API_KEY}'
```

### Full Installation (All Features)
```bash
# Install all features including visualizations
pip install -r requirements.txt
# Includes: pandas, numpy, scipy, matplotlib for full statistical analysis
```

## üõ†Ô∏è Scripts Overview

**Note**: This is a completed research project. Scripts are provided for replication and further analysis. The main evaluation has been completed with all 4 models.

This project includes four main scripts for different research tasks:

### **run_research.py** - Full Evaluation Pipeline
**Purpose**: Comprehensive evaluation of all 4 models with statistical analysis and visualizations.

```bash
# Quick evaluation (3 scenarios, ~3 minutes) - For replication
python scripts/run_research.py --quick --output results/development/

# Full evaluation (10 scenarios, ~10 minutes) - COMPLETED
python scripts/run_research.py

# View completed results (recommended)
cat results/research_report.txt
open results/visualizations/1_overall_comparison.png

# Replicate with custom settings
python scripts/run_research.py --scenarios 5 --output results/development/
```

**Key Options** (for replication studies):
- `--quick` - Fast 3-scenario evaluation for testing
- `--all-models` - Use all 4 models (COMPLETED - see results/)
- `--models openai,claude` - Specify subset of models
- `--minimal` - Clean output for presentations
- `--output results/development/` - Use for new experiments (n=10 study complete)

### **compare_models.py** - Quick Model Comparison
**Purpose**: Fast comparison of models on single prompts with immediate results.

```bash
# Compare models on single prompt
python scripts/compare_models.py "How can I manage anxiety?"

# Interactive mode for multiple prompts
python scripts/compare_models.py --interactive

# Use all 4 models
python scripts/compare_models.py "Hello" --all-models

# Batch mode from file
python scripts/compare_models.py --batch prompts.txt --all-models
```

**Key Options**:
- `--interactive` - Interactive prompt mode
- `--all-models` - Use all 4 models
- `--models openai,claude` - Specify models
- `--batch file.txt` - Process multiple prompts from file

### **demo_presentation.py** - Academic Demo Tool
**Purpose**: Clean demonstration script optimized for live presentations and capstone defense.

```bash
# Default demo (anxiety + crisis scenarios)
python scripts/demo_presentation.py

# Specific scenarios
python scripts/demo_presentation.py --scenario anxiety depression

# All scenarios with specific models
python scripts/demo_presentation.py --scenario all --models openai,claude
```

**Key Options**:
- `--scenario {anxiety,depression,crisis,all}` - Choose demo scenarios
- `--models` - Specify models to compare
- Clean, professional output (no emojis or debug info)
- Quick execution (< 2 minutes)

### **clean_results.py** - Results Maintenance
**Purpose**: Organize and clean the results directory for presentations and archival.

```bash
# Preview cleanup actions
python scripts/clean_results.py --dry-run

# Create presentation backup only
python scripts/clean_results.py --backup

# Remove temporary files only
python scripts/clean_results.py --clean

# Full cleanup (backup + clean)
python scripts/clean_results.py --full
```

**Key Options**:
- `--dry-run` - Preview changes without modifying files
- `--backup` - Create organized presentation backup
- `--clean` - Remove temporary/development files
- `--full` - Complete cleanup (recommended)

## üöÄ Quick Start Examples

```bash
# Complete research evaluation
python scripts/run_research.py --quick

# Quick model comparison
python scripts/compare_models.py "I'm feeling anxious about work"

# Live demo for presentation
python scripts/demo_presentation.py

# Clean up results folder
python scripts/clean_results.py --full
```

### View Completed Results
Final research results available in `results/` directory:
- **Statistical analysis** with effect sizes and confidence intervals (`statistical_analysis.json`)
- **5 Publication-quality visualizations** showing all 4 models:
  1. `1_overall_comparison.png` - Performance comparison across all metrics  
  2. `2_category_radar.png` - Radar chart showing model strengths
  3. `3_cost_effectiveness.png` - Cost vs performance analysis
  4. `4_safety_metrics.png` - Safety-focused analysis 
  5. `5_statistical_summary.png` - Statistical significance summary
- **Research report** with methodology and findings (`research_report.txt`)
- **Executive summary** with key findings (`RESULTS_SUMMARY.md`)
- **Detailed evaluation data** for further analysis (`detailed_results.json`)

**All visualizations display all 4 models** with proper color coding:
- OpenAI GPT-4 (blue) - 6.82/10
- Claude (green) - 5.41/10  
- DeepSeek (orange) - 7.90/10 **WINNER**
- Gemma (purple) - 4.10/10

## üìä Evaluation Framework

### Therapeutic Quality Metrics (NEW WEIGHTS)
- **Safety** (35%): Crisis handling and appropriate mental health responses
- **Empathy** (30%): Emotional understanding and validation
- **Therapeutic Value** (25%): Helpful coping strategies and techniques
- **Clarity** (10%): Clear, understandable responses

### Performance Metrics
- Response time and latency
- **Cost efficiency**: OpenAI $0.021/conversation vs DeepSeek $0.00
- Token usage and optimization
- Error rates and availability
- Scalability considerations

### Statistical Analysis Features
- **Normality testing** (Shapiro-Wilk) for appropriate test selection
- **Significance testing** (t-test for normal, Mann-Whitney U for non-normal)
- **Effect size calculation** (Cohen's d with interpretation)
- **Confidence intervals** (95% CI for all metrics)
- **Safety violation tracking** with detailed categorization

## üß™ Mental Health Scenarios

**10 Validated Scenarios** across key categories:
- **Anxiety Disorders**: General anxiety, work-related panic, social anxiety
- **Depression**: Persistent mood issues, seasonal depression, energy loss
- **Crisis Situations**: Suicidal ideation, self-harm urges, acute distress
- **General Support**: Relationship stress, grief, academic pressure, sleep issues

Each scenario includes:
- Realistic patient presentations
- Expected therapeutic responses
- Safety considerations
- Scoring rubrics

## üìà Statistical Analysis & Approach

**Exploratory Study Design**:
- **Sample Size**: n=10 scenarios per model (pilot study scope)
- **Focus**: Effect sizes and confidence intervals rather than p-values
- **Models**: All 4 models evaluated (GPT-4, Claude, DeepSeek, Gemma)
- **Analysis Type**: Descriptive statistics with practical significance assessment

**Statistical Methods**:
- **Automated test selection** based on normality (Shapiro-Wilk)
- **Effect size calculation** (Cohen's d) with interpretation as primary metric
- **95% confidence intervals** for all measurements
- **Detailed metric breakdowns** by therapeutic dimension
- **Safety violation tracking** with categorical analysis

**Publication-Ready Outputs**:
- **Automated visualization generation** (300 DPI publication quality)
- **Research report generation** with methodology documentation
- **Effect size interpretations** (small: 0.2, medium: 0.5, large: 0.8+)
- **Comprehensive model comparison** across all therapeutic dimensions

## üîß Configuration (Simplified)

**No configuration files needed!** All settings have sensible defaults.

Optional configuration files in `config/` directory:
- `config/scenarios.yaml` - 10 mental health evaluation scenarios
- `config/config.yaml` - Main configuration (model settings, evaluation weights)
- `config/capstone_config.yaml` - Capstone-specific settings

**Environment Variables**:
- `OPENAI_API_KEY` - Required for OpenAI GPT-4 access
- All other settings have working defaults

## üìù Usage Examples

```bash
# Quick research run (3 scenarios)
python run_research.py --quick

# Full research evaluation (10 scenarios)
python run_research.py

# Custom scenario count
python run_research.py --scenarios 7

# Custom output location
python run_research.py --output my_study_results

# Quick model comparison (legacy)
python quick_comparison.py "I'm feeling anxious about work"

# Interactive testing mode (legacy)
python quick_comparison.py --interactive
```

## üîÆ Future Expansion Plans

**Additional Models** (architecture ready):
- **Gemini Pro** (Google) - Multimodal support
- **Llama 3** (Meta) - Open-source alternative
- **Local alternatives** - Mistral, Qwen, etc.

**Enhanced Features**:
- Extended scenario library (50+ scenarios)
- Advanced statistical methods (ANOVA, regression)
- Optional web interface for easier access
- Multi-language support
- Conference publication preparation
- **Multiple response averaging** for increased reliability

**Future Research Directions**:
- **Scale to n=50+ scenarios** for increased statistical power
- **Multi-language evaluation** for international healthcare applications  
- **Real-world deployment study** with healthcare professionals
- **Longitudinal analysis** of model consistency over time
- **Integration with clinical decision support systems**

## üéì Academic Research Value

**Primary Research Questions**:
1. **Therapeutic Effectiveness**: How do local/open-source LLMs (DeepSeek, Gemma) compare to commercial cloud LLMs (GPT-4, Claude) in therapeutic conversation quality?
2. **Multi-Model Comparison**: What are the relative strengths and weaknesses across all 4 models?
3. **Cost-Benefit Analysis**: What are the performance vs cost trade-offs across cloud and local deployment options?
4. **Safety Assessment**: How do models handle crisis situations and maintain appropriate therapeutic boundaries?

**Research Methodology**:
- **Exploratory pilot study** design with n=10 validated scenarios
- **Effect size analysis** (Cohen's d) as primary statistical metric
- **4-model comparison** framework (GPT-4, Claude, DeepSeek, Gemma)
- **Therapeutic quality assessment** with weighted scoring system
- **Cost-effectiveness modeling** comparing cloud vs local deployment
- **Publication-quality outputs** suitable for academic submission

**Research Status - COMPLETED**:
- **Phase 1 Complete**: Framework development and initial testing ‚úÖ
- **Phase 2 Complete**: Data collection across all 10 scenarios for 4 models ‚úÖ
- **Phase 3 Complete**: Final statistical analysis and publication-ready results ‚úÖ
- **Phase 4 Ready**: Academic publication and presentation materials prepared

## üìä Completed Deliverables

1. **Comparative Analysis Report** - Complete with statistical findings (`research_report.txt`) ‚úÖ
2. **Publication-Ready Results** - Academic format with methodology documented ‚úÖ
3. **Healthcare Recommendations** - DeepSeek recommended for therapeutic applications ‚úÖ
4. **Open Source Framework** - Extensible evaluation platform available ‚úÖ
5. **Statistical Analysis** - Significance testing with large effect sizes confirmed ‚úÖ
6. **Cost-Effectiveness Analysis** - Local models provide superior value proposition ‚úÖ

## üõ°Ô∏è Safety & Ethics

**Research Ethics**:
- This is a research evaluation tool, not for actual mental health treatment
- Crisis scenarios included for academic evaluation purposes only
- All outputs should be reviewed by qualified mental health professionals
- Appropriate disclaimers included in all model responses

**Data Privacy**:
- No personal health information collected
- Synthetic scenarios used for evaluation
- Local model option for privacy-sensitive deployments
- Results stored locally, not transmitted to external services
- **OpenAI API usage** follows standard data retention policies

**Study Limitations & Future Work**:
- **10 scenario pilot study** - suitable for exploratory research, larger studies recommended
- **Single response per scenario** - multiple runs could improve reliability
- **Local model setup** - requires technical expertise for DeepSeek/Gemma deployment
- **Statistical power** - larger sample sizes would strengthen confidence intervals
- **English language only** - multilingual evaluation needed for global applications

## üéØ Target Audience

**Primary Users**:
- **Academic Researchers** - Mental health AI research and evaluation
- **Healthcare Organizations** - AI deployment decision-making
- **AI/ML Developers** - Therapeutic application development
- **Graduate Students** - Applied AI research projects

**Use Cases**:
- Comparative LLM research for healthcare
- Cost-benefit analysis for AI deployment
- Safety assessment of therapeutic AI systems
- Academic capstone and thesis projects

## üìû Support & Contributing

**Getting Help**:
- Review troubleshooting section in documentation
- Check existing GitHub issues
- Submit new issues with detailed problem description

**Contributing**:
- Fork repository and create feature branches
- Follow existing code style and documentation standards
- Submit pull requests with clear descriptions
- Include tests for new functionality

---

## üìà Project Transformation Summary

**Before**: 1,740 files, enterprise complexity, overwhelming scope
**After**: 22 core files, research-focused, immediate usability
**Achievement**: 99% reduction while maintaining 100% research functionality

**Recent Cleanup (January 2025)**:
- **Organized structure**: Configuration files in `config/`, tests in `tests/`, demos in `demos/`
- **Consolidated dependencies**: `requirements.txt` (full) + `requirements_minimal.txt`
- **Removed enterprise archive**: Eliminated 98 unnecessary files
- **Clean main directory**: Only essential files at root level

**Key Success Factors**:
- **Academic focus** over enterprise features
- **Streamlined architecture** with logical organization
- **Publication-ready statistical analysis** with automated reporting
- **5-minute setup** for immediate research use
- **Single command execution** with sensible defaults
- **Publication-quality visualizations** for academic presentation

**Final Project Status (July 2025)**:
- **Research study completed** with statistical significance achieved
- **All 4 models evaluated** across 10 therapeutic scenarios
- **Publication-ready results** with comprehensive analysis
- **DeepSeek identified as superior performer** for therapeutic applications
- **Academic presentation materials** prepared and ready

*Final Project: Research complete, 4 models evaluated, statistical significance achieved, publication-ready*

## üßπ Recent Refactoring (July 2025)

### Major Cleanup Completed
- **Reduced src/ from 43 to 25 essential files** (42% reduction)
- **Implemented hierarchical configuration system** with environment overrides
- **Archived enterprise complexity** while maintaining all core functionality
- **Fixed import structure** and improved code organization
- **Created tools directory** for standalone utilities

### Key Improvements
- ‚úÖ **Faster imports** and reduced memory footprint
- ‚úÖ **Clearer code organization** with logical module grouping
- ‚úÖ **Simplified dependency graph** and import patterns
- ‚úÖ **Comprehensive documentation** with consistent structure
- ‚úÖ **Preserved research capabilities** while removing complexity

### File Structure
```
src/                    # 25 core files (was 43)
‚îú‚îÄ‚îÄ models/            # 4 files - OpenAI, DeepSeek, Local, Base
‚îú‚îÄ‚îÄ evaluation/        # 3 files - Core evaluation logic
‚îú‚îÄ‚îÄ analysis/          # 3 files - Statistics & visualization
‚îú‚îÄ‚îÄ config/            # 3 files - Configuration management
‚îú‚îÄ‚îÄ scenarios/         # 3 files - Scenario processing
‚îî‚îÄ‚îÄ utils/             # 2 files - Utilities

config/                # 17 organized configuration files
‚îú‚îÄ‚îÄ environments/      # Dev/prod environment configs
‚îú‚îÄ‚îÄ models/           # Model-specific settings
‚îú‚îÄ‚îÄ evaluation/       # Evaluation metrics
‚îî‚îÄ‚îÄ scenarios/        # Test scenarios

tools/                 # Standalone utilities
‚îî‚îÄ‚îÄ compare_models.py  # Model comparison tool

_archive/             # 152 archived files
‚îú‚îÄ‚îÄ src/              # 13 specialized modules
‚îî‚îÄ‚îÄ original/         # 139 original files
```

See `FILE_DESCRIPTIONS_UPDATED.md` for detailed structure documentation.


