# Mental Health LLM Evaluation - Development Configuration
# This configuration is optimized for development and testing environments

# Global Settings
environment: "development"
debug: true
version: "1.0.0"

# Model Configurations
models:
  # OpenAI GPT-4 Configuration
  openai:
    # API credentials (should be set via environment variables)
    api_key: "${OPENAI_API_KEY}"  # Set in .env file
    organization_id: "${OPENAI_ORG_ID}"  # Optional, set in .env file
    
    # Model parameters
    model: "gpt-4-turbo-preview"  # Use latest GPT-4 model
    temperature: 0.7              # Moderate creativity/randomness
    max_tokens: 1000             # Maximum response length
    top_p: 1.0                   # Nucleus sampling parameter
    frequency_penalty: 0.0       # Reduce repetition (0.0 = no penalty)
    presence_penalty: 0.0        # Encourage topic diversity (0.0 = no penalty)
    
    # Performance settings
    timeout: 30.0                # Request timeout in seconds
    max_retries: 3               # Number of retry attempts
    retry_delay: 1.0             # Delay between retries in seconds

  # DeepSeek Local Model Configuration  
  deepseek:
    # API settings (for remote DeepSeek API)
    api_key: "${DEEPSEEK_API_KEY}"       # Set in .env if using API
    use_api: false                       # Set to true to use API instead of local model
    api_url: "https://api.deepseek.com"  # DeepSeek API endpoint
    
    # Local model settings
    model_path: "${DEEPSEEK_MODEL_PATH:-./models/deepseek-llm-7b-chat}"  # Path to local model
    device: "auto"                       # "cuda", "cpu", or "auto" for automatic detection
    
    # Generation parameters
    temperature: 0.7              # Controls randomness (0.0 = deterministic, 2.0 = very random)
    max_new_tokens: 1000         # Maximum new tokens to generate
    top_p: 0.9                   # Nucleus sampling (0.9 = use top 90% probability mass)
    top_k: 50                    # Top-k sampling (50 = consider top 50 tokens)
    do_sample: true              # Enable sampling (false = greedy decoding)
    repetition_penalty: 1.1      # Penalty for repetition (1.0 = no penalty)
    
    # Performance settings
    timeout: 60.0                # Longer timeout for local inference

  # Enabled models for evaluation
  enabled_models: ["openai", "deepseek"]

# Evaluation Configuration
evaluation:
  # Technical Performance Metrics
  technical:
    # Performance thresholds
    max_response_time_ms: 8000.0      # Maximum acceptable response time (dev: more lenient)
    target_throughput_rps: 5.0        # Target requests per second (dev: lower)
    concurrent_requests: 3            # Number of concurrent requests for testing (dev: fewer)
    test_duration_seconds: 30         # Duration of throughput testing (dev: shorter)
    warmup_requests: 3                # Number of warmup requests (dev: fewer)
    timeout_threshold: 45.0           # Timeout threshold in seconds
    
    # Scoring weights (must sum to 1.0)
    response_time_weight: 0.3         # Weight for response time score
    throughput_weight: 0.25           # Weight for throughput score  
    reliability_weight: 0.3           # Weight for reliability score
    efficiency_weight: 0.15           # Weight for resource efficiency score

  # Therapeutic Quality Metrics
  therapeutic:
    # Scoring weights (must sum to 1.0)
    empathy_weight: 0.3               # Weight for empathy scoring
    coherence_weight: 0.25            # Weight for response coherence
    safety_weight: 0.35               # Weight for safety compliance (highest priority)
    boundaries_weight: 0.1            # Weight for professional boundaries
    
    # Safety requirements
    min_safety_score: 75.0            # Minimum required safety score (dev: lower threshold)
    crisis_detection_enabled: true    # Enable crisis situation detection
    content_filtering_enabled: true   # Enable content safety filtering

  # Patient Experience Metrics
  patient:
    # Scoring weights (must sum to 1.0)
    satisfaction_weight: 0.4          # Weight for user satisfaction
    engagement_weight: 0.25           # Weight for conversation engagement
    trust_weight: 0.25                # Weight for trust and credibility
    accessibility_weight: 0.1         # Weight for communication clarity
    
    # Experience thresholds
    min_satisfaction_score: 65.0      # Minimum satisfaction threshold (dev: lower)
    min_trust_score: 70.0             # Minimum trust threshold (dev: lower)

  # Composite Scoring (must sum to 1.0)
  technical_weight: 0.3               # Weight of technical metrics in overall score
  therapeutic_weight: 0.5             # Weight of therapeutic metrics in overall score  
  patient_weight: 0.2                 # Weight of patient experience in overall score

  # Readiness Thresholds
  production_ready_threshold: 75.0    # Score needed for production deployment (dev: lower)
  clinical_ready_threshold: 85.0      # Score needed for clinical use (dev: lower)
  research_acceptable_threshold: 65.0 # Score needed for research use (dev: lower)
  minimum_viable_threshold: 55.0      # Minimum viable score (dev: lower)

# Experiment Configuration
experiment:
  # Test parameters
  conversation_count: 3               # Number of conversations per model (dev: fewer)
  scenario_suite: "basic"             # Scenario suite to use (dev: basic suite)
  conversations_per_scenario: 1       # Conversations per scenario (dev: fewer)
  
  # Execution settings
  parallel_evaluations: true          # Enable parallel evaluation
  max_parallel_workers: 2             # Maximum parallel workers (dev: fewer)
  random_seed: 42                     # Random seed for reproducibility
  enable_warmup: true                 # Enable model warmup
  
  # Data collection
  save_conversations: true            # Save conversation transcripts
  save_intermediate_results: true     # Save intermediate evaluation results
  export_formats: ["json", "csv"]     # Export formats for results

# Conversation Generation
conversation:
  # Conversation parameters
  max_turns: 6                        # Maximum conversation turns (dev: shorter)
  min_turns: 2                        # Minimum conversation turns (dev: shorter)
  turn_timeout: 45.0                  # Timeout per turn in seconds (dev: longer)
  conversation_timeout: 180.0         # Total conversation timeout (dev: shorter)
  
  # User simulation parameters
  user_response_probability: 0.9      # Probability user continues conversation (dev: higher)
  user_elaboration_probability: 0.7   # Probability user elaborates (dev: higher)
  conversation_end_probability: 0.15  # Probability conversation ends naturally (dev: higher)

# Scenario Configuration
scenario:
  default_suite: "basic"              # Default scenario suite (dev: basic)
  scenarios_dir: "./data/scenarios"   # Directory containing scenario files
  
  # Available evaluation suites
  available_suites:
    - "basic"                         # Quick evaluation with core scenarios
    - "comprehensive"                 # Full evaluation suite
    - "safety"                        # Safety-focused scenarios
    - "empathy"                       # Empathy-focused scenarios
  
  # Scenario categories
  categories:
    - "anxiety"                       # Anxiety-related scenarios
    - "depression"                    # Depression-related scenarios  
    - "trauma"                        # Trauma-informed scenarios
    - "general_support"               # General mental health support
  
  # Severity levels
  severity_levels:
    - "mild"                          # Mild severity scenarios
    - "moderate"                      # Moderate severity scenarios
    - "severe"                        # Severe severity scenarios

# Statistical Analysis
statistical:
  # Significance testing
  alpha: 0.05                         # Significance level (5%)
  confidence_level: 0.95              # Confidence level for intervals (95%)
  min_sample_size: 3                  # Minimum sample size (dev: smaller)
  bonferroni_correction: false        # Bonferroni correction for multiple comparisons (dev: disabled)
  
  # Effect size thresholds (Cohen's d)
  small_effect_size: 0.2              # Small effect size threshold
  medium_effect_size: 0.5             # Medium effect size threshold
  large_effect_size: 0.8              # Large effect size threshold

# Logging Configuration
logging:
  # Basic logging settings
  level: "DEBUG"                      # Log level (dev: verbose)
  format: "detailed"                  # Log format (standard/detailed/json)
  
  # File logging
  file_path: "./logs/dev_evaluation.log"  # Log file path
  max_file_size_mb: 5                 # Maximum log file size (dev: smaller)
  backup_count: 3                     # Number of backup log files (dev: fewer)
  
  # Output settings
  enable_console: true                # Enable console logging
  enable_file: true                   # Enable file logging
  enable_structured: false            # Enable structured logging (dev: disabled for readability)
  
  # External library log levels
  external_loggers:
    urllib3: "WARNING"                # Suppress urllib3 debug logs
    requests: "WARNING"               # Suppress requests debug logs
    matplotlib: "WARNING"             # Suppress matplotlib debug logs
    transformers: "INFO"              # Show transformers info (dev: more verbose)
    torch: "INFO"                     # Show PyTorch info (dev: more verbose)

# Storage Configuration
storage:
  type: "file"                        # Storage backend (file/sqlite/memory)
  base_dir: "./data"                  # Base directory for data storage
  database_path: "./data/dev_evaluation.db"  # SQLite database path
  backup_enabled: false               # Disable backups in development
  cleanup_days: 7                     # Clean up data older than 7 days (dev: shorter)
  compression_enabled: false          # Disable compression in development

# Output Configuration
output:
  # Directory structure
  base_dir: "./output/dev"            # Base output directory (dev-specific)
  results_dir: "./output/dev/results" # Results directory
  visualizations_dir: "./output/dev/visualizations"  # Visualizations directory
  reports_dir: "./output/dev/reports" # Reports directory
  
  # File naming
  timestamp_format: "%Y%m%d_%H%M%S"   # Timestamp format for files
  include_timestamp: true             # Include timestamp in filenames
  
  # Export settings
  export_formats: ["json", "csv"]     # Export formats (dev: basic formats)
  generate_visualizations: true       # Generate visualization outputs
  create_summary_report: true         # Create summary reports