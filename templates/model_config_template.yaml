# Model Configuration Template
# Template for adding new models to experiment configuration

# INSTRUCTIONS:
# 1. Copy this template section to your main experiment configuration
# 2. Replace placeholder values with your model's specific parameters
# 3. Update enabled: true when your model is ready
# 4. Test with: python scripts/model_management.py validate-config your-config.yaml

experiment:
  name: "Mental Health LLM Evaluation with New Models"
  description: "Evaluation including newly added models"
  version: "2.0.0"
  allow_partial_models: true  # Allow some models to fail during testing

models:
  # Cloud-based models (API-driven)
  cloud:
    # Template for API-based models (OpenAI, Anthropic, Google, etc.)
    - name: "your-cloud-model"          # Replace with your model name
      provider: "your-provider"         # Replace with provider name
      enabled: false                   # Set to true when ready
      
      # Model-specific API parameters
      model: "your-model-id"           # API model identifier
      temperature: 0.7                 # Creativity/randomness (0.0-2.0)
      max_tokens: 1000                 # Maximum response length
      top_p: 0.9                       # Nucleus sampling parameter
      timeout: 30.0                    # API timeout in seconds
      
      # Optional advanced parameters
      frequency_penalty: 0.0           # Penalize repeated words
      presence_penalty: 0.0            # Penalize repeated topics
      max_retries: 3                   # Number of retry attempts
      retry_delay: 1.0                 # Delay between retries
      
      # Cost management (optional)
      max_cost_per_request: 0.10       # Maximum cost per request (USD)
      daily_cost_limit: 10.00          # Daily spending limit (USD)
    
    # Example: Anthropic Claude configuration
    - name: "claude-3-opus"
      provider: "anthropic"
      enabled: false                   # Enable when ANTHROPIC_API_KEY is set
      model: "claude-3-opus-20240229"
      temperature: 0.7
      max_tokens: 1000
      timeout: 30.0
      max_retries: 3
    
    # Example: Google Gemini configuration  
    - name: "gemini-pro"
      provider: "google"
      enabled: false                   # Enable when GOOGLE_API_KEY is set
      model: "gemini-pro"
      temperature: 0.7
      max_tokens: 1000
      timeout: 30.0
    
    # Example: OpenAI GPT-4 (already configured)
    - name: "gpt-4"
      provider: "openai"
      enabled: true
      model: "gpt-4-turbo-preview"
      temperature: 0.7
      max_tokens: 1000
      timeout: 30.0

  # Local models (self-hosted)
  local:
    # Template for local models (Llama, Mistral, custom models, etc.)
    - name: "your-local-model"          # Replace with your model name
      provider: "your-provider"         # Replace with provider name
      enabled: false                   # Set to true when model is downloaded
      
      # Model file parameters
      model_path: "./models/your-model" # Path to model files/directory
      device: "auto"                   # "auto", "cuda", "cuda:0", "cpu"
      precision: "fp16"                # "fp16", "bf16", "fp32"
      
      # Generation parameters
      temperature: 0.7                 # Creativity/randomness (0.0-2.0)
      max_tokens: 1000                 # Maximum response length
      top_p: 0.9                       # Nucleus sampling parameter
      top_k: 50                        # Top-k sampling parameter
      repetition_penalty: 1.1          # Penalize repetition
      
      # Memory optimization (for large models)
      load_in_8bit: false              # Enable 8-bit quantization
      load_in_4bit: false              # Enable 4-bit quantization
      low_cpu_mem_usage: true          # Optimize CPU memory usage
      trust_remote_code: false         # Allow custom model code
      
      # Advanced parameters
      use_cache: true                  # Enable KV cache for faster inference
      torch_dtype: "auto"              # PyTorch data type
      attention_implementation: "sdpa"  # Attention implementation
    
    # Example: Llama-2 7B configuration
    - name: "llama-2-7b-chat"
      provider: "meta"
      enabled: false                   # Enable when model is downloaded
      model_path: "./models/llama-2-7b-chat-hf"
      device: "auto"
      precision: "fp16"
      temperature: 0.7
      max_tokens: 1000
      top_p: 0.9
      repetition_penalty: 1.1
      load_in_8bit: false
      load_in_4bit: false
    
    # Example: Llama-2 13B configuration (with quantization)
    - name: "llama-2-13b-chat"
      provider: "meta"
      enabled: false
      model_path: "./models/llama-2-13b-chat-hf"
      device: "auto"
      precision: "fp16"
      temperature: 0.7
      max_tokens: 1000
      load_in_8bit: true               # Enable 8-bit for memory efficiency
      load_in_4bit: false
    
    # Example: DeepSeek configuration (already configured)
    - name: "deepseek"
      provider: "deepseek"
      enabled: true
      model_path: "./models/deepseek-llm-7b-chat"
      device: "auto"
      temperature: 0.7
      max_new_tokens: 1000
      use_api: false
    
    # Example: Mistral configuration
    - name: "mistral-7b-instruct"
      provider: "mistral"
      enabled: false
      model_path: "./models/mistral-7b-instruct-v0.2"
      device: "auto"
      precision: "fp16"
      temperature: 0.7
      max_tokens: 1000

# Scenario configuration
scenarios:
  directory: "data/scenarios"
  include: []                          # Specific scenarios or [] for all
  category: []                         # Filter by category or [] for all
  severity: []                         # Filter by severity or [] for all

# Evaluation configuration
evaluation:
  conversations_per_scenario: 10       # Number of conversations per scenario per model
  max_conversation_turns: 20           # Maximum turns per conversation
  max_concurrent: 3                    # Concurrent conversations (be careful with local models)
  timeout_seconds: 300                 # Timeout for conversation generation
  enable_safety_monitoring: true       # Enable safety flag detection
  enable_metrics_collection: true      # Collect performance metrics
  
  # Evaluation frameworks
  empathy:
    enabled: true
    weights:
      emotional_recognition: 0.3
      perspective_taking: 0.3
      compassionate_response: 0.4
      
  safety:
    enabled: true
    crisis_detection: true
    harmful_content_detection: true
    
  coherence:
    enabled: true
    context_awareness: true
    logical_consistency: true
    
  therapeutic:
    enabled: true
    technique_recognition: true
    boundary_maintenance: true
    
  composite:
    enabled: true
    weights:
      empathy: 0.3
      safety: 0.4
      coherence: 0.3

# Output configuration
output:
  base_directory: "./experiments"
  conversations: "conversations"
  evaluations: "evaluations"
  results: "results"
  reports: "reports"

# Analysis configuration
analysis:
  statistical_tests:
    - "anova"                          # Analysis of variance
    - "t_test"                         # T-tests for pairwise comparisons
    - "effect_size"                    # Effect size calculations
    - "correlation"                    # Correlation analysis
  
  visualizations:
    - "box_plots"                      # Distribution comparisons
    - "heatmaps"                       # Correlation matrices
    - "radar_charts"                   # Multi-dimensional comparisons
    - "scatter_plots"                  # Relationship analysis
    - "model_comparison"               # Side-by-side model comparisons
    - "time_series"                    # Performance over time
  
  export_formats:
    - "png"                            # High-quality images
    - "pdf"                            # Publication-ready PDFs
    - "svg"                            # Scalable vector graphics

# Report generation configuration
reports:
  formats:
    - "html"                           # Interactive web reports
    - "pdf"                            # Printable PDF reports
    - "markdown"                       # Documentation format
  
  include_sections:
    - "executive_summary"              # High-level findings
    - "methodology"                    # Experimental setup
    - "results"                        # Detailed results
    - "statistical_analysis"          # Statistical comparisons
    - "model_comparison"               # Model-by-model analysis
    - "safety_analysis"                # Safety findings
    - "recommendations"                # Actionable insights
    - "limitations"                    # Study limitations
    - "conclusions"                    # Final conclusions
  
  template: "academic"                 # Report style: "academic", "clinical", "executive"

# Environment variables needed (document in comments)
# Cloud models typically require API keys:
# export OPENAI_API_KEY=your-openai-key
# export ANTHROPIC_API_KEY=your-anthropic-key  
# export GOOGLE_API_KEY=your-google-key

# Local models require model files:
# Download models to the paths specified in model_path
# Ensure sufficient GPU memory for enabled local models

# Hardware requirements by model type:
# - GPT-4, Claude: Internet connection, API credits
# - Llama-2 7B: 14GB GPU memory (fp16) or 7GB (8-bit)
# - Llama-2 13B: 26GB GPU memory (fp16) or 13GB (8-bit)
# - Llama-2 70B: 140GB GPU memory (fp16) or 35GB (4-bit)